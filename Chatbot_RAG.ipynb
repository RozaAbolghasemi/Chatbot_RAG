{"cells":[{"cell_type":"raw","metadata":{"id":"rUdw_4grpiOo"},"source":["import sys\n","print(sys.executable)\n"]},{"cell_type":"raw","metadata":{"id":"JN-8klRCpiOp"},"source":["!pip install pdfplumber\n"]},{"cell_type":"raw","metadata":{"id":"jdT9DUklpiOp"},"source":["!python3 -m pip install ipykernel\n","!python3 -m ipykernel install --user --name=python3.8 --display-name \"Python 3.8 (my_env)\"\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9490,"status":"ok","timestamp":1734970281077,"user":{"displayName":"Roza Abolghasemi","userId":"12806178772948633973"},"user_tz":-60},"id":"wkDnCe7jp8D7","outputId":"031cb0fc-d369-4095-c743-d2d01d7d8492"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n","Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n","Requirement already satisfied: Pillow\u003e=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n","Requirement already satisfied: pypdfium2\u003e=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.1)\n","Requirement already satisfied: charset-normalizer\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228-\u003epdfplumber) (3.4.0)\n","Requirement already satisfied: cryptography\u003e=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228-\u003epdfplumber) (43.0.3)\n","Requirement already satisfied: cffi\u003e=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography\u003e=36.0.0-\u003epdfminer.six==20231228-\u003epdfplumber) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi\u003e=1.12-\u003ecryptography\u003e=36.0.0-\u003epdfminer.six==20231228-\u003epdfplumber) (2.22)\n"]}],"source":["pip install pdfplumber"]},{"cell_type":"markdown","metadata":{"id":"NkL7JuDbpiOq"},"source":["# Step 1: Extract Text from PDFs"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1486,"status":"ok","timestamp":1734970282560,"user":{"displayName":"Roza Abolghasemi","userId":"12806178772948633973"},"user_tz":-60},"id":"oK3uxXsJ6wZ2","outputId":"dc797043-4abd-4a47-ea47-fe0fa39a2c05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"RzzYHUaYuMQc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting text extraction from PDF files...\n","Extracted text from BMW_Annual_Report_2023.pdf\n","Extracted text from BMW_Annual_Report_2022.pdf\n","Extracted text from BMW_Annual_Report_2021.pdf\n","Extracted text from Tesla_Annual_Report_2023.pdf\n","Extracted text from Tesla_Annual_Report_2022.pdf\n","Extracted text from Ford_Annual_Report_2023.pdf\n","Extracted text from Ford_Annual_Report_2022.pdf\n"]}],"source":["import pdfplumber\n","import os\n","\n","# Define the folder where your PDF files are stored\n","pdf_folder_path = \"/content/drive/MyDrive/Data_Consigli\"  # Folder path containing the PDF files\n","\n","# Function to extract text from a PDF file\n","def extract_text_from_pdf(pdf_path):\n","    try:\n","        with pdfplumber.open(pdf_path) as pdf:\n","            text = ''\n","            for page in pdf.pages:\n","                text += page.extract_text() or ''\n","        return text\n","    except Exception as e:\n","        print(f\"Error reading {pdf_path}: {e}\")\n","        return None\n","\n","# Extract text from all PDF files in the folder\n","pdf_texts = {}\n","print(\"Starting text extraction from PDF files...\")\n","for root, dirs, files in os.walk(pdf_folder_path):\n","    for file in files:\n","        if file.endswith('.pdf'):\n","            file_path = os.path.join(root, file)\n","            pdf_texts[file] = extract_text_from_pdf(file_path)\n","            print(f\"Extracted text from {file}\")\n","\n","# Save extracted text to a local dictionary or separate files for review\n","print(\"\\nExtraction completed. Sample content from extracted PDFs:\\n\")\n","for file_name, text in pdf_texts.items():\n","    if text:\n","        # Print a sample of the extracted text\n","        print(f\"Sample content from {file_name}:\\n{text[:1000]}\\n{'-'*80}\\n\")\n","    else:\n","        print(f\"No text extracted from {file_name}.\\n{'-'*80}\\n\")\n","\n","print(\"Text extraction process finished. All files processed.\")"]},{"cell_type":"markdown","metadata":{"id":"PGiwBqytpiOs"},"source":["# Step 2: Clean and Prepare Data"]},{"cell_type":"markdown","metadata":{"id":"LVECAZ2gpiOs"},"source":["Once the text is extracted, you need to preprocess it to remove unnecessary whitespace, headers, and footers. Use this code for preprocessing:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzcbEiCapiOs"},"outputs":[],"source":["import re\n","\n","def clean_text(text):\n","    # Remove extra whitespace and newlines\n","    text = re.sub(r'\\s+', ' ', text)\n","    # Remove special characters if not needed\n","    text = re.sub(r'[^\\w\\s.,]', '', text)\n","    return text.strip()\n","\n","# Clean all extracted texts\n","cleaned_texts = {file_name: clean_text(text) for file_name, text in pdf_texts.items() if text}\n","\n","# Save cleaned text into files for later use\n","output_folder = \"cleaned_texts\"\n","os.makedirs(output_folder, exist_ok=True)\n","\n","for file_name, text in cleaned_texts.items():\n","    output_path = os.path.join(output_folder, file_name.replace('.pdf', '.txt'))\n","    with open(output_path, 'w', encoding='utf-8') as f:\n","        f.write(text)\n","    print(f\"Cleaned text saved to {output_path}\")\n"]},{"cell_type":"markdown","metadata":{"id":"KVdLA3l1piOt"},"source":["# Step 3: Index the Data for RAG"]},{"cell_type":"markdown","metadata":{"id":"H30SzXIopiOt"},"source":["We'll use FAISS (Facebook AI Similarity Search) to index the cleaned text data. FAISS is a library for efficient similarity search and clustering of dense vectors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-jXr0jwpiOt"},"outputs":[],"source":["pip install faiss-cpu"]},{"cell_type":"markdown","metadata":{"id":"QC3K3aIPpiOu"},"source":["Convert Text to Embeddings\n","We need to convert the cleaned text into embeddings. We'll use a pre-trained model from the sentence-transformers package to generate these embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8M5jgQFpiOu"},"outputs":[],"source":["!pip install sentence-transformers"]},{"cell_type":"markdown","metadata":{"id":"MRAL8JUMpiOv"},"source":["Python Code for Indexing\n","Here's the Python code to convert text into embeddings and index them using FAISS:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ovpvr1CLpiOv"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","import faiss\n","import numpy as np\n","import os\n","\n","# Load the pre-trained model\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Function to read cleaned text files\n","def read_cleaned_texts(folder_path):\n","    texts = {}\n","    for file_name in os.listdir(folder_path):\n","        if file_name.endswith('.txt'):\n","            file_path = os.path.join(folder_path, file_name)\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                texts[file_name] = f.read()\n","    return texts\n","\n","# Read the cleaned texts\n","cleaned_text_folder = \"cleaned_texts\"\n","cleaned_texts = read_cleaned_texts(cleaned_text_folder)\n","\n","# Debugging print statements\n","print(f\"Number of documents read: {len(cleaned_texts)}\")\n","\n","# Generate embeddings for each document\n","document_names = list(cleaned_texts.keys())\n","document_texts = list(cleaned_texts.values())\n","\n","# Check if document_texts is empty\n","if not document_texts:\n","    print(\"No documents found in the specified folder.\")\n","else:\n","    # Generate embeddings\n","    document_embeddings = model.encode(document_texts)\n","\n","    # Debugging print statements\n","    print(f\"Shape of document_embeddings: {document_embeddings.shape}\")\n","\n","    # Create a FAISS index\n","    dimension = document_embeddings.shape[1]\n","    faiss_index = faiss.IndexFlatL2(dimension)\n","    faiss_index.add(np.array(document_embeddings))\n","\n","    # Save the index and document names for later use\n","    faiss.write_index(faiss_index, \"document_faiss_index.index\")\n","    with open(\"document_names.txt\", 'w', encoding='utf-8') as f:\n","        for name in document_names:\n","            f.write(name + \"\\n\")\n","\n","    print(\"Index created and saved successfully.\")"]},{"cell_type":"markdown","metadata":{"id":"5GBVtuivpiOw"},"source":["# Step 4: Implement the LLM-Powered QA System"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtP6DMgD5acZ"},"outputs":[],"source":["pip install openai #== 0.28"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"CXo-TqijxVxA"},"outputs":[],"source":["import openai\n","import faiss\n","from sentence_transformers import SentenceTransformer\n","import numpy as np\n","\n","# Load the pre-trained model and FAISS index\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","faiss_index = faiss.read_index(\"document_faiss_index.index\")\n","\n","# Load document names\n","with open(\"document_names.txt\", 'r', encoding='utf-8') as f:\n","    document_names = f.read().splitlines()\n","\n","# Set OpenAI API key\n","openai.api_key = \"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\"\n","\n","# Function to perform the retrieval\n","def retrieve_relevant_documents(query, top_k=5):\n","    query_embedding = model.encode([query])\n","    distances, indices = faiss_index.search(np.array(query_embedding), top_k)\n","    return [(document_names[idx], distances[0][i]) for i, idx in enumerate(indices[0])]\n","\n","# Function to truncate context to fit within the token limit\n","def truncate_context(context, max_tokens=1000):\n","    words = context.split()\n","    if len(words) \u003e max_tokens:\n","        words = words[:max_tokens]\n","    return ' '.join(words)\n","\n","# Function to generate an answer using GPT\n","def generate_answer(query, context):\n","    truncated_context = truncate_context(context)\n","    response = openai.ChatCompletion.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": f\"Context: {truncated_context}\\n\\nQuestion: {query}\\nAnswer:\"}\n","        ],\n","        max_tokens=150\n","    )\n","    return response['choices'][0]['message']['content'].strip()\n","\n","# Interactive QA system\n","def interactive_qa():\n","    while True:\n","        query = input(\"Enter your question (or type 'exit' to quit): \")\n","        if query.lower() == 'exit':\n","            break\n","        relevant_docs = retrieve_relevant_documents(query)\n","        context = \" \".join([cleaned_texts[doc[0]] for doc in relevant_docs])\n","        answer = generate_answer(query, context)\n","        print(f\"Answer: {answer}\\n\")\n","\n","# Start the interactive QA system\n","interactive_qa()"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python (myenv)","language":"python","name":"myenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"}},"nbformat":4,"nbformat_minor":0}